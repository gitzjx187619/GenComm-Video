
# GenComm-Video: LLM-Guided Semantic Video Communication ğŸš€
## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/gitzjx187619/GenComm-Video.git
   cd GenComm-Video
   ```

2. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

   *Recommendation: A GPU with at least 6GB VRAM (NVIDIA RTX 3060 or higher).*

3. **download modules**
   
   ```bash
   models--lllyasviel--sd-controlnet-canny
   models--runwayml--stable-diffusion-v1-5
   models--Salesforce--blip-image-captioning-base
   ```
---

## ğŸš€ Quick Start

1. **Prepare Input Video**:
   Place your test video (e.g., `test.mp4`) in the root directory.
   *Recommended resolution: 540p or 720p. The code will auto-resize it for AI stability.*

2. **Run the Pipeline**:

   ```bash
   python main.py
   ```

3. **Check Outputs**:

   * `output_generative.mp4`: The AI-reconstructed high-fidelity video.
   * `result_plot.png`: The R-D comparison curve against H.264.

---


## ğŸ“‚ Project Structure

```text
GenComm-Video/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ encoder.py          # VLM Semantic Extraction & Edge Detection
â”‚   â”œâ”€â”€ decoder.py          # Generative Reconstruction (SD + ControlNet)
â”‚   â””â”€â”€ utils.py            # Video I/O helpers
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ simulate_channel.py # Bitrate calculation & Bit-packing simulation
â”‚   â””â”€â”€ compare_metrics.py  # LPIPS evaluation & Plotting
â”œâ”€â”€ main.py                 # Main entry point
â””â”€â”€ requirements.txt        # Python dependencies
```

